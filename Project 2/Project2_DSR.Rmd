---
title: "Kepler Exoplanet Search Project"
author: "Data Science Rookies"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
   # number_sections: True
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---
  
```{r basic, include=F}
# use this function to conveniently load libraries and work smoothly with knitting
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }

# unload/detact package when done using it
unloadPkg = function(pkg, character.only = FALSE) { 
  if(!character.only) { pkg <- as.character(substitute(pkg)) } 
  search_item <- paste("package", pkg,sep = ":") 
  while(search_item %in% search()) { detach(search_item, unload = TRUE, character.only = TRUE) } 
}
```

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r xkablesummary}
loadPkg("xtable")
loadPkg("kableExtra")
loadPkg("stringi")

xkabledply = function(modelsmmrytable, title="Table", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display model summary. 
  #' wrapper for the base::summary function on model objects
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param modelsmmrytable This can be a generic table, a model object such as lm(), or the summary of a model object summary(lm()) 
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return HTML table for display
  #' @examples
  #' library("xt height = "300px"able")
  #' library("kableExtra")
  #' xkabledply( df, title="Table testing", pos="left", bso="hover" )
  modelsmmrytable %>%
    xtable() %>% 
    kable(caption = title, digits = digits) %>%
    kable_styling(bootstrap_options = bso, full_width = FALSE, position = pos) %>%
    kableExtra::scroll_box(width = "500px")
}

xkablesummary = function(df, title="Table: Statistics summary.", digits = 4, pos="left", bso="striped") { 
  #' Combining base::summary, xtable, and kableExtra, to easily display numeric variable summary of dataframes. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param df The dataframe.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @return The HTML summary table for display, or for knitr to process into other formats 
  #' @examples
  #' xkablesummary( faraway::ozone )
  #' xkablesummary( ISLR::Hitters, title="Five number summary", pos="left", bso="hover"  )
  
  s = summary(df) %>%
    apply( 2, function(x) stringr::str_remove_all(x,c("Min.\\s*:\\s*","1st Qu.\\s*:\\s*","Median\\s*:\\s*","Mean\\s*:\\s*","3rd Qu.\\s*:\\s*","Max.\\s*:\\s*")) ) %>% # replace all leading words
    apply( 2, function(x) stringr::str_trim(x, "right")) # trim trailing spaces left
  
  colnames(s) <- stringr::str_trim(colnames(s))
  
  if ( dim(s)[1] ==6 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max') 
  } else if ( dim(s)[1] ==7 ) { rownames(s) <- c('Min','Q1','Median','Mean','Q3','Max','NA') }
  
  xkabledply(s, title=title, digits = digits, pos=pos, bso=bso )
}

xkablevif = function(model, title="VIFs of the model", digits = 3, pos="left", bso="striped", wide=FALSE) { 
  #' Combining faraway::vif, xtable, and kableExtra, to easily display numeric summary of VIFs for a model. 
  #' ELo 202004 GWU DATS
  #' version 1.2
  #' @param model The lm or compatible model object.
  #' @param title Title of table. 
  #' @param digits Number of digits to display
  #' @param pos Position of table, c("left","center","right") 
  #' @param bso bootstrap_options = c("basic", "striped", "bordered", "hover", "condensed", "responsive")
  #' @param wide print table in long (FALSE) format or wide (TRUE) format
  #' @return The HTML summary table of the VIFs for a model for display, or for knitr to process into other formats 
  #' @examples
  #' xkablevif( lm(Salary~Hits+RBI, data=ISLR::Hitters, wide=T ) )
  
  vifs = table( names(model$coefficients)[2:length(model$coefficients)] ) # remove intercept to set column names
  vifs[] = faraway::vif(model) # set the values
  if (wide) { vifs <- t(vifs) }
  xkabledply( vifs, title=title, digits = digits, pos=pos, bso=bso )
}

```

# 1 Introduction  

Have you ever wondered what life on other planets might be like? If so, you are not alone. Humans have been looking to the sky and beyond for centuries wondering what might be found in the vast landscape of the universe. As technology advances, we continue developing tools that allow us to better identify objects of interest beyond the scope of our galaxy, one of those tools being the Kepler Space Telescope (KST). The KST is a space telescope that was launched into Earth’s orbit on March 7, 2009 to monitor more than 150,000 stars in search for transiting exoplanets. All objects the KST shows interest in are labeled as Kepler Objects of Interest (KOI), and the KOI feature data is logged and stored by NASA. This data is available to the public and is what we will be analyzing.This paper aims to explore the idea of using classification techniques, specifically logistic regression and k-nearest neighbor, to accurately distinguish between exoplanet and non-exoplanet KOIs.

**The SMART questions we will be focusing on are:**  
1. For the 9,564 KOIs in the database, can we use classification methods to accurately classify KOIs as exoplanets based on the given feature data?  
2. Which will be more accurate for classifying KOIs, a logistic regression or k-nearest neighbor model?

Included in our discussion is some exploratory data analysis (EDA) and preprocessing of the dataset to prepare the data to run through our models.

# 2 EDA 

Checklist for EDA:
- Check for normality
- Check class balance (count plot of koi-disposition)
- Correlation heatmap

```{r, read files}
koi_df <- data.frame(read.csv('Kepler_Exoplanet.csv'))
length(koi_df)
```
```{r, remove unwanted columns}

koi_df1<-data.frame(subset(koi_df,select = -c(rowid,kepid,kepoi_name,kepler_name,koi_period_err1
,koi_period_err2,koi_time0bk_err1,koi_time0bk_err2,koi_impact_err1,koi_impact_err2,koi_duration_err1,koi_duration_err2,koi_depth_err1,koi_depth_err2,koi_prad_err1,koi_prad_err2,koi_teq_err1,koi_teq_err2,koi_insol_err1,koi_insol_err2,koi_steff_err1,koi_steff_err2,koi_slogg_err1,koi_slogg_err2,koi_srad_err1,koi_srad_err2,koi_tce_delivname)))

length(koi_df1)
```


```{R}
str(koi_df1)
```

```{R, remove missing values}
koi_df1 = na.exclude(koi_df1)
xkablesummary(koi_df1[c(3, 8:23)], title = "Summary of the numeric")

```


## Count Plot

```{R}
loadPkg("ggplot2")
count_koi_disposition = ggplot(data =  koi_df1, aes(x = koi_disposition)) +
  geom_bar(fill = c("grey"), col = c("blue", "red", "green")) +
   geom_text(stat = "count", aes(label = ..count..)) + 
    labs(title = "Count plot of koi_disposition")
count_koi_disposition   

count_koi_pdisposition = ggplot(data =  koi_df1, aes(x = koi_pdisposition)) +
  geom_bar(fill = c("grey"), col = c("blue", "red")) +
   geom_text(stat = "count", aes(label = ..count..)) + 
    labs(title = "Count plot of koi_pdisposition")
count_koi_pdisposition
```


```{R, change the data type}
koi_df1$koi_disposition <- ifelse(koi_df1$koi_disposition == "FALSE POSITIVE", 0,
                     ifelse(koi_df1$koi_disposition == "CONFIRMED", 1, 2))

koi_df1$koi_pdisposition[koi_df1$koi_pdisposition == "FALSE POSITIVE"] <- 0
koi_df1$koi_pdisposition[koi_df1$koi_pdisposition == "CANDIDATE"] <- 1

koi_df1$koi_disposition = as.factor(koi_df1$koi_disposition)
koi_df1$koi_pdisposition = as.factor(koi_df1$koi_pdisposition)

str(koi_df1)
```
## Normality Test
```{R, normality test}
shapiro.test(koi_df1$koi_period[0:5000])
qqnorm(koi_df1$koi_period, pch = 1, frame = FALSE, main = "QQ-plot of koi period")
qqline(koi_df1$koi_period, col = "blue", lwd = 2)

# I am not sure which variables we are gonna use, just doing one for a sample. 
# koi_period, I don't know what exactly you are. But you suck.
```

```{R}
## We can do anova or chi-squared test or feature selection method for testing the impact of the important variables. 

## The correlation is not needed here?


```

## Train-Test split

```{r Train-Test Split }
set.seed(1000)
koi_sample <- sample(2, nrow(koi_df1), replace=TRUE, prob=c(0.75, 0.25))
koi_train_dis <- koi_df1[koi_sample==1, -2]     # X and Y=koi_disposition
koi_test_dis <- koi_df1[koi_sample==2, -2]      # X and Y=koi_disposition
koi_train_pdis <- koi_df1[koi_sample==1, -1]    # X and Y=koi_pdisposition
koi_test_pdis <- koi_df1[koi_sample==2, -1]     # X and Y=koi_pdisposition
koi_trainX <- koi_df1[koi_sample==1, 3:23]      # Only X 
koi_testX <- koi_df1[koi_sample==2, 3:23]       # Only X
koi_trainLabels_dis <- koi_df1[koi_sample==1, 1]  # Only Y= koi_disposition
koi_testLabels_dis <- koi_df1[koi_sample==2, 1]   # Only Y= koi_disposition
koi_trainLabels_pdis <- koi_df1[koi_sample==1, 2] # Only Y= koi_pdisposition
koi_testLabels_pdis <- koi_df1[koi_sample==2, 2]  # Only Y= koi_pdisposition

```

## Multi-Logistic for "koi_disposition"

```{r Multi-Logistic of disposition}
loadPkg("nnet")
loadPkg("caret")
loadPkg("pROC")
logmod_koi <- multinom(koi_disposition~.,data = koi_train_dis,model = T)
logmod_sum <- summary(logmod_koi)
logmod_pred <- predict(logmod_koi,koi_test_dis)
mean(logmod_pred==koi_testLabels_dis)
# The prediction accuracy of the model is 0.833
logmod_cm <- confusionMatrix(logmod_pred,koi_testLabels_dis)
logmod_cm
# confusion matrix
```

## K-NN for "koi_disposition"

```{r KNN of disposition}
loadPkg("FNN")

chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(1) # seems no meaning
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k) #,                #<- number of neighbors considered
                  # use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  
  tab = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(tab[row(tab) == col(tab)]) / sum(tab)                         
  cbind(k = k, accuracy = accu)
}

knn_different_k <- sapply(seq(1, 21, by = 2),
                          function(x) chooseK(x, 
                                             train_set = koi_trainX,
                                             val_set = koi_testX,
                                             train_class = koi_trainLabels_dis,
                                             val_class = koi_testLabels_dis))

knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])
knn_different_k
# K= 17 has the best accuracy 0.638, still worse than multi-logistic 

knn_pred <- knn(train = koi_trainX,test=koi_testX,cl = koi_trainLabels_dis,k = 17)
knn_cm <- confusionMatrix(knn_pred,koi_testLabels_dis)
knn_cm     # confusion matrix for 17-NN

```




